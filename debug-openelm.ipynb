{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "from quiet_star.config import Config, ModelConfig\n",
    "from quiet_star.torch.openelm import OpenELMThoughtModel\n",
    "from quiet_star.torch.pretrained import PretrainedThoughtModel\n",
    "from quiet_star.torch.qwen import QwenThoughtModel\n",
    "from quiet_star.torch.utils import torch_dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = Config(\n",
    "    batch_size=2,\n",
    "    lookahead_tokens=3,\n",
    "    thought_length=3,\n",
    "    model=ModelConfig(\n",
    "        attn_type=\"torch\",\n",
    "        dtype=\"float32\",\n",
    "        device=device,\n",
    "        dropout_attn=0.0,\n",
    "        dropout_embed=0.0,\n",
    "        model_name=\"apple/OpenELM-270M-Instruct\",\n",
    "        tokenizer_name=\"meta-llama/Llama-2-7b-hf\",\n",
    "        max_length=32,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model.model_name,\n",
    "    torch_dtype=torch_dtype(config.model.dtype),\n",
    "    trust_remote_code=True,\n",
    ").to(config.model.device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model.tokenizer_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "text = \"This is a longer test sentence.\"\n",
    "x = tokenizer(\n",
    "    text,\n",
    "    padding=\"do_not_pad\",\n",
    "    truncation=True,\n",
    "    max_length=config.model.max_length - config.thought_length - 2,\n",
    "    return_tensors=\"np\",\n",
    "    return_attention_mask=False,\n",
    ")[\"input_ids\"][0].tolist()\n",
    "input_ids = torch.tensor(\n",
    "    [x for _ in range(config.batch_size)],\n",
    "    dtype=torch.int64,\n",
    "    device=config.model.device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of OpenELMForCausalLM(\n",
       "  (transformer): OpenELMModel(\n",
       "    (token_embeddings): Embedding(32000, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (proj_2): Linear(in_features=768, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (1): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2048, bias=False)\n",
       "          (proj_2): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (2): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=2560, bias=False)\n",
       "          (proj_2): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (3): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3072, bias=False)\n",
       "          (proj_2): Linear(in_features=1536, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (4): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=12, key_heads=3, value_heads=3\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1152, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=3584, bias=False)\n",
       "          (proj_2): Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (5): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (proj_2): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (6): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5120, bias=False)\n",
       "          (proj_2): Linear(in_features=2560, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (7): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=5632, bias=False)\n",
       "          (proj_2): Linear(in_features=2816, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (8): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6144, bias=False)\n",
       "          (proj_2): Linear(in_features=3072, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (9): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=6656, bias=False)\n",
       "          (proj_2): Linear(in_features=3328, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (10): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (proj_2): Linear(in_features=3584, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (11): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=16, key_heads=4, value_heads=4\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1536, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=7680, bias=False)\n",
       "          (proj_2): Linear(in_features=3840, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (12): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=8704, bias=False)\n",
       "          (proj_2): Linear(in_features=4352, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (13): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9216, bias=False)\n",
       "          (proj_2): Linear(in_features=4608, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (14): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=9728, bias=False)\n",
       "          (proj_2): Linear(in_features=4864, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "      (15): OpenELMDecoderLayer(\n",
       "        (attn): OpenELMMultiHeadCausalAttention(\n",
       "          query_heads=20, key_heads=5, value_heads=5\n",
       "          (qkv_proj): Linear(in_features=1280, out_features=1920, bias=False)\n",
       "          (pos_embedding): OpenELMRotaryEmbedding(\tmodel_dim=64, max_seq_length=4096, freq_constant=10000)\n",
       "          (q_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (k_norm): OpenELMRMSNorm(num_features=64, eps=1e-06)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        )\n",
       "        (ffn): OpenELMFeedForwardNetwork(\n",
       "          (ffn_with_glu) : True\n",
       "          (proj_1): Linear(in_features=1280, out_features=10240, bias=False)\n",
       "          (proj_2): Linear(in_features=5120, out_features=1280, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (ffn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "        (attn_norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): OpenELMRMSNorm(num_features=1280, eps=1e-06)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.transformer.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8]),\n",
       " tensor([[    1,   910,   338,   263,  5520,  1243, 10541, 29889],\n",
       "         [    1,   910,   338,   263,  5520,  1243, 10541, 29889]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 32000]),\n",
       " tensor([[[ -9.9827,  -9.4185,  -1.3738,  ...,  -5.5373,  -6.8383,  -4.9822],\n",
       "          [-11.7295, -20.6063,  -5.0599,  ...,  -8.0301,  -9.4964,  -8.1932],\n",
       "          [-11.1467, -12.0488,  -4.3880,  ...,  -6.7239,  -8.3881,  -7.5842],\n",
       "          ...,\n",
       "          [-10.9954, -18.0083,  -1.4600,  ...,  -9.1341, -11.1416,  -9.1180],\n",
       "          [-11.8424, -17.7331,  -4.0620,  ..., -11.7066, -11.5651,  -7.8645],\n",
       "          [-11.9498, -12.1675,  -7.3315,  ...,  -6.3045,  -8.0009,  -4.4673]],\n",
       " \n",
       "         [[ -9.9827,  -9.4185,  -1.3738,  ...,  -5.5373,  -6.8383,  -4.9822],\n",
       "          [-11.7295, -20.6063,  -5.0599,  ...,  -8.0301,  -9.4964,  -8.1932],\n",
       "          [-11.1467, -12.0488,  -4.3880,  ...,  -6.7239,  -8.3881,  -7.5842],\n",
       "          ...,\n",
       "          [-10.9954, -18.0083,  -1.4600,  ...,  -9.1341, -11.1416,  -9.1180],\n",
       "          [-11.8424, -17.7331,  -4.0620,  ..., -11.7066, -11.5651,  -7.8645],\n",
       "          [-11.9498, -12.1675,  -7.3315,  ...,  -6.3045,  -8.0009,  -4.4673]]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids, output_hidden_states=True)\n",
    "outputs[0].shape, outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3351e-03, 6.4850e-04, 1.4067e-05,  ..., 1.8616e-03,\n",
       "          8.2016e-04, 7.1335e-04]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.token_embeddings(torch.tensor([[1]], dtype=torch.int64, device=model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1280]),\n",
       " tensor([[ 1.3351e-03,  6.4850e-04,  1.4067e-05,  ...,  1.8616e-03,\n",
       "           8.2016e-04,  7.1335e-04],\n",
       "         [ 1.3306e-02,  3.4943e-03,  4.4434e-02,  ...,  3.1250e-02,\n",
       "           4.6631e-02,  1.3306e-02],\n",
       "         [-5.0781e-02,  2.6245e-02,  5.4932e-03,  ..., -4.9805e-02,\n",
       "          -1.2268e-02, -2.9663e-02],\n",
       "         ...,\n",
       "         [-2.1729e-02, -2.2705e-02,  2.3804e-02,  ..., -2.7710e-02,\n",
       "           3.2959e-02, -1.4771e-02],\n",
       "         [ 1.6357e-02, -3.6377e-02,  6.2561e-03,  ...,  2.4902e-02,\n",
       "           1.0071e-02, -8.5449e-02],\n",
       "         [-1.4725e-03,  1.6235e-02,  2.7954e-02,  ..., -1.3550e-02,\n",
       "           3.4485e-03, -2.5513e-02]], device='cuda:0',\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][0][0].shape, outputs[2][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/src/quiet-star/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 278.12M\n"
     ]
    }
   ],
   "source": [
    "tmodel = OpenELMThoughtModel(config).to(config.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 1280]),\n",
       " tensor([[[ 1.3351e-03,  6.4850e-04,  1.4067e-05,  ...,  1.8616e-03,\n",
       "            8.2016e-04,  7.1335e-04],\n",
       "          [ 1.3306e-02,  3.4943e-03,  4.4434e-02,  ...,  3.1250e-02,\n",
       "            4.6631e-02,  1.3306e-02],\n",
       "          [-5.0781e-02,  2.6245e-02,  5.4932e-03,  ..., -4.9805e-02,\n",
       "           -1.2268e-02, -2.9663e-02],\n",
       "          ...,\n",
       "          [-2.1729e-02, -2.2705e-02,  2.3804e-02,  ..., -2.7710e-02,\n",
       "            3.2959e-02, -1.4771e-02],\n",
       "          [ 1.6357e-02, -3.6377e-02,  6.2561e-03,  ...,  2.4902e-02,\n",
       "            1.0071e-02, -8.5449e-02],\n",
       "          [-1.4725e-03,  1.6235e-02,  2.7954e-02,  ..., -1.3550e-02,\n",
       "            3.4485e-03, -2.5513e-02]],\n",
       " \n",
       "         [[ 1.3351e-03,  6.4850e-04,  1.4067e-05,  ...,  1.8616e-03,\n",
       "            8.2016e-04,  7.1335e-04],\n",
       "          [ 1.3306e-02,  3.4943e-03,  4.4434e-02,  ...,  3.1250e-02,\n",
       "            4.6631e-02,  1.3306e-02],\n",
       "          [-5.0781e-02,  2.6245e-02,  5.4932e-03,  ..., -4.9805e-02,\n",
       "           -1.2268e-02, -2.9663e-02],\n",
       "          ...,\n",
       "          [-2.1729e-02, -2.2705e-02,  2.3804e-02,  ..., -2.7710e-02,\n",
       "            3.2959e-02, -1.4771e-02],\n",
       "          [ 1.6357e-02, -3.6377e-02,  6.2561e-03,  ...,  2.4902e-02,\n",
       "            1.0071e-02, -8.5449e-02],\n",
       "          [-1.4725e-03,  1.6235e-02,  2.7954e-02,  ..., -1.3550e-02,\n",
       "            3.4485e-03, -2.5513e-02]]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, l = input_ids.shape\n",
    "\n",
    "causal_mask1 = torch.triu(\n",
    "    torch.full((l, l), float(\"-inf\"), dtype=tmodel._dtype, device=tmodel.device),\n",
    "    diagonal=1,\n",
    ")\n",
    "causal_mask1 = causal_mask1.unsqueeze(0).unsqueeze(1)\n",
    "causal_mask1 = causal_mask1.repeat(b, 1, 1, 1)\n",
    "\n",
    "row = torch.arange(0, l, dtype=torch.int64, device=tmodel.device)\n",
    "position_ids = row.reshape(1, l).tile((b, 1))\n",
    "\n",
    "x = tmodel.tok_emb(input_ids)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x[0], outputs[2][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1280]),\n",
       " tensor([[ 0.0274,  0.0157,  0.0946,  ..., -0.0227,  0.0035, -0.0185],\n",
       "         [-0.0532, -0.1701,  0.0188,  ...,  0.1279,  0.1166,  0.0664],\n",
       "         [-0.4543, -0.0032, -0.1225,  ...,  0.1885, -0.0846, -0.0783],\n",
       "         ...,\n",
       "         [-0.1314, -0.1227, -0.0984,  ..., -0.0571,  0.1851,  0.0105],\n",
       "         [-0.2809,  0.0842,  0.7183,  ...,  0.0822,  0.1731,  0.1994],\n",
       "         [ 0.0046, -0.0328, -0.0318,  ...,  0.0520,  0.0103,  0.0787]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][1][0].shape, outputs[2][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1280]),\n",
       " tensor([[ 0.0274,  0.0157,  0.0946,  ..., -0.0227,  0.0035, -0.0185],\n",
       "         [-0.0532, -0.1701,  0.0188,  ...,  0.1279,  0.1166,  0.0664],\n",
       "         [-0.4543, -0.0032, -0.1225,  ...,  0.1885, -0.0846, -0.0783],\n",
       "         ...,\n",
       "         [-0.1314, -0.1227, -0.0984,  ..., -0.0571,  0.1851,  0.0105],\n",
       "         [-0.2809,  0.0842,  0.7183,  ...,  0.0822,  0.1731,  0.1994],\n",
       "         [ 0.0046, -0.0328, -0.0318,  ...,  0.0520,  0.0103,  0.0787]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, layer = 0, tmodel.layers[0]\n",
    "\n",
    "residual = x\n",
    "x2 = layer.attn_norm(x)\n",
    "\n",
    "attn_out = layer.attn(x2, attention_mask=causal_mask1)[0]\n",
    "\n",
    "x2 = residual + attn_out\n",
    "x2 = x2 + layer.ffn(layer.ffn_norm(x2))\n",
    "\n",
    "x2[0].shape, x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1280]),\n",
       " tensor([[ 0.0274,  0.0157,  0.0946,  ..., -0.0227,  0.0035, -0.0185],\n",
       "         [-0.0532, -0.1701,  0.0188,  ...,  0.1279,  0.1166,  0.0664],\n",
       "         [-0.4543, -0.0032, -0.1225,  ...,  0.1885, -0.0846, -0.0783],\n",
       "         ...,\n",
       "         [-0.1314, -0.1227, -0.0984,  ..., -0.0571,  0.1851,  0.0105],\n",
       "         [-0.2809,  0.0842,  0.7183,  ...,  0.0822,  0.1731,  0.1994],\n",
       "         [ 0.0046, -0.0328, -0.0318,  ...,  0.0520,  0.0103,  0.0787]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "i, layer = 0, tmodel.layers[0]\n",
    "\n",
    "residual = x\n",
    "x2 = layer.attn_norm(x)\n",
    "\n",
    "qkv = (\n",
    "    layer.attn.qkv_proj(x2)\n",
    "    .reshape(b, l, 2 * tmodel.num_kv_heads[i] + tmodel.num_query_heads[i], -1)\n",
    "    .swapaxes(1, 2)\n",
    ")\n",
    "q, k, v = qkv.split([tmodel.num_query_heads[i], tmodel.num_kv_heads[i], tmodel.num_kv_heads[i]], dim=1)\n",
    "\n",
    "q = layer.attn.q_norm(q)\n",
    "k = layer.attn.k_norm(k)\n",
    "\n",
    "# apply rotary embedding\n",
    "q, k = layer.attn.pos_embedding(q, k)\n",
    "# cos, sin = layer.self_attn.rotary_emb(v, seq_len=l)\n",
    "# q = self.apply_rotary_pos_emb(q, cos, sin, position_ids)\n",
    "# k = self.apply_rotary_pos_emb(k, cos, sin, position_ids)\n",
    "\n",
    "print(layer.attn.num_groups, tmodel.num_gqa_groups)\n",
    "k = k.repeat_interleave(layer.attn.num_groups, dim=1)\n",
    "v = v.repeat_interleave(layer.attn.num_groups, dim=1)\n",
    "\n",
    "a = torch.nn.functional.softmax(\n",
    "    (torch.matmul(q, k.transpose(-2, -1)) + causal_mask1)\n",
    "    / math.sqrt(q.size(-1)),\n",
    "    dim=-1,\n",
    ")\n",
    "\n",
    "# attn_out is (B, H, L, E)\n",
    "attn_out = torch.matmul(a, v)\n",
    "\n",
    "attn_out = layer.attn.out_proj(\n",
    "    attn_out.permute([0, 2, 1, 3]).reshape(b, l, tmodel.num_query_heads[i] * tmodel.head_dim)\n",
    ")\n",
    "x2 = residual + attn_out\n",
    "x2 = x2 + layer.ffn(layer.ffn_norm(x2))\n",
    "\n",
    "x2[0].shape, x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class OpenELMMultiHeadCausalAttention(nn.Module):\n",
      "    def __init__(self, config: OpenELMConfig, layer_idx: int) -> None:\n",
      "        super().__init__()\n",
      "        self.layer_idx = layer_idx\n",
      "        head_dim = config.head_dim\n",
      "        q_heads = config.num_query_heads[layer_idx]\n",
      "        k_heads = config.num_kv_heads[layer_idx]\n",
      "        v_heads = config.num_kv_heads[layer_idx]\n",
      "\n",
      "        self.qkv_proj = nn.Linear(\n",
      "            in_features=config.model_dim,\n",
      "            out_features=(q_heads + k_heads + v_heads) * head_dim,\n",
      "            bias=False,\n",
      "        )\n",
      "\n",
      "        self.pos_embedding = OpenELMRotaryEmbedding(\n",
      "            model_dim=config.head_dim,\n",
      "            max_seq_length=config.rope_max_length,\n",
      "            freq_constant=config.rope_freq_constant,\n",
      "        )\n",
      "\n",
      "        if config.normalize_qk_projections:\n",
      "            self.q_norm = OpenELMRMSNorm(\n",
      "                num_features=config.head_dim,\n",
      "            )\n",
      "            self.k_norm = OpenELMRMSNorm(\n",
      "                num_features=config.head_dim,\n",
      "            )\n",
      "        else:\n",
      "            self.q_norm = None\n",
      "            self.k_norm = None\n",
      "\n",
      "        self.out_proj = nn.Linear(\n",
      "            in_features=q_heads * head_dim,\n",
      "            out_features=config.model_dim,\n",
      "            bias=False,\n",
      "        )\n",
      "\n",
      "        self.head_dim = config.head_dim\n",
      "        self.num_q_heads = q_heads\n",
      "        self.num_k_heads = k_heads\n",
      "        self.num_v_heads = v_heads\n",
      "        self.transformer_dim = config.model_dim\n",
      "        self.num_groups = self.num_q_heads // self.num_k_heads\n",
      "\n",
      "    def extra_repr(self) -> str:\n",
      "        return (\n",
      "            super().extra_repr()\n",
      "            + f\"query_heads={self.num_q_heads}, key_heads={self.num_k_heads}, value_heads={self.num_v_heads}\"\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        \"\"\"\n",
      "        Forward pass of multi-head self-attention.\n",
      "\n",
      "        Args:\n",
      "            hidden_states: Input tensor of the shape [batch size, sequence length, model dimension].\n",
      "            past_key_value: Tensor storing the cached keys and values.\n",
      "            output_attentions: output attention weights.\n",
      "            use_cache: Specifies whether to use kv-cache for generation.\n",
      "            cache_position: used for updating the kv-cache.\n",
      "\n",
      "        Returns:\n",
      "            The output of the same shape as the input, optionally with a tensor containing cached keys and values.\n",
      "        \"\"\"\n",
      "\n",
      "        # scaled_dot_product_attention does not return attention weights, set output_attentions to False\n",
      "        output_attentions = False\n",
      "        batch_size, seq_length, d_model = hidden_states.size()\n",
      "\n",
      "        # [B, S, d] --> [B, S, (q_h + k_h + v_h) * h]\n",
      "        qkv = self.qkv_proj(hidden_states)\n",
      "        # [B, S, (q_h + k_h + v_h) * h] --> [B, S, (q_h + k_h + v_h), h]\n",
      "        qkv = qkv.reshape(\n",
      "            batch_size,\n",
      "            seq_length,\n",
      "            self.num_q_heads + self.num_k_heads + self.num_v_heads,\n",
      "            self.head_dim,\n",
      "        )\n",
      "        # [B, S, (q_h + k_h + v_h), h] --> [B, (q_h + k_h + v_h), S, h]\n",
      "        qkv = qkv.transpose(1, 2)\n",
      "        # [B, (q_h + k_h + v_h), S, h] --> [B, q_h, S h], [B, k_h, S, h], [B, v_h, S, h]\n",
      "        queries, keys, values = qkv.split(\n",
      "            [self.num_q_heads, self.num_k_heads, self.num_v_heads], dim=1\n",
      "        )\n",
      "\n",
      "        if self.q_norm is not None:\n",
      "            queries = self.q_norm(queries)\n",
      "\n",
      "        if self.k_norm is not None:\n",
      "            keys = self.k_norm(keys)\n",
      "\n",
      "        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n",
      "            # cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            cache_kwargs = {\"cache_position\": cache_position}\n",
      "            keys, values = past_key_value.update(\n",
      "                keys, values, self.layer_idx, cache_kwargs\n",
      "            )\n",
      "\n",
      "        # Add positional embedding\n",
      "        queries, keys = self.pos_embedding(queries, keys)\n",
      "\n",
      "        if self.num_groups != 1:\n",
      "            # GQA\n",
      "            # [B, k_h, S, h] --> [B, q_h, S, h]\n",
      "            keys = keys.repeat_interleave(self.num_groups, dim=1)\n",
      "            # [B, v_h, S, h] --> [B, q_h, S, h]\n",
      "            values = values.repeat_interleave(self.num_groups, dim=1)\n",
      "\n",
      "        causal_mask = attention_mask\n",
      "        if attention_mask is not None and cache_position is not None:\n",
      "            causal_mask = causal_mask[:, :, cache_position, : keys.shape[-2]]\n",
      "\n",
      "        attn_output = F.scaled_dot_product_attention(\n",
      "            queries,\n",
      "            keys,\n",
      "            values,\n",
      "            attn_mask=causal_mask,\n",
      "            dropout_p=0,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(\n",
      "            batch_size, seq_length, self.num_q_heads * self.head_dim\n",
      "        )\n",
      "        attn_output = self.out_proj(attn_output)\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(layer.attn.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers_modules.apple.OpenELM-270M-Instruct.eb111ff2e6724348e5b905984063d4064d4bc579.modeling_openelm'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.attn.__class__.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
